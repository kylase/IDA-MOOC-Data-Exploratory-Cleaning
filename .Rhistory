install.packages("tm")
ls
ls()
library(tm)
setwd("~/Documents/Projects/IDA_MOOC_data_cleaning_exploratory")
library(tools)
str1_md5 = md5sum(file.path(getwd(), 'str1.txt'))
str2_md5 = md5sum(file.path(getwd(), 'str2.txt'))
# Check if hashes are the same
str1_md5 == str2_md5
str1_md5 = md5sum(file.path(getwd(), 'str1.txt'))
str2_md5 = md5sum(file.path(getwd(), 'str2.txt'))
str1_md5 == str2_md5
str1_md5
library(tm)
sample = c('There is “something” going on right now, like right… now. He’s there.',
'There is "something" going on right now, like right... now. He\'s there.')
corpus = VCorpus(VectorSource(sample), readerControl = list(reader = readPlain))
# NLP's Bigram Tokeniser
NLPBigramTokenizer <- function(x) {
unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
dtm_nlp = DocumentTermMatrix(corpus, control = list(tokenize = NLPBigramTokenizer))
inspect(dtm_nlp)
dtm_nlp_mat = as.data.frame(colSums(as.matrix(dtm_nlp)))
# RWeka's Bigram Tokeniser
library(RWeka)
WekaBigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
dtm_weka = DocumentTermMatrix(corpus, control = list(tokenize = WekaBigramTokenizer))
inspect(dtm_weka)
dtm_weka_mat = as.data.frame(colSums(as.matrix(dtm_weka)))
